# Each card is identified by a unique keyword, followed by these fields:
#   title: The title of the card
#   tags: An array of tags to categorize the card
#   website: The website url
#   video: The youtube video url
#   paper: The paper/arXiv url
#   repo: The github repository url
#   notes: One-liner comment about the paper

ReKep:
  title: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation
  tags: [FeiFei, policy, hierarchical, OmniGibson]
  paper: https://arxiv.org/abs/2409.01652
  website: https://rekep-robot.github.io/
  repo: https://github.com/huangwl18/ReKep
  notes: Large vision models and vision-language models can generate keypoint-based constraints, which can be optimized to achieve multi-stage, in-the-wild, bimanual, and reactive behaviors, without task-specific training or environment models.

RoboCasa:
  title: Large-Scale Simulation of Everyday Tasks for Generalist Robots
  tags: [NVIDIA, YukeZhu, benchmark, dataset, RoboSuite]
  paper: https://arxiv.org/abs/2406.02523
  website: https://robocasa.ai/
  repo: https://github.com/robocasa/robocasa
  notes: 2,500 3D assets across 150+ object categories and dozens of interactable furniture and appliances. A suite of 100 everyday activities with 100k demonstrations. 

# MimicGen:
#   title: A Data Generation System for Scalable Robot Learning using Human Demonstrations
#   tags: [NVIDIA, DieterFox, YukeZhu, dataset, RoboSuite]
#   paper: https://arxiv.org/abs/2310.17596
#   website: https://mimicgen.github.io/
#   repo: https://github.com/NVlabs/mimicgen
#   notes: We used MimicGen to autonomously generate over 50,000 demonstrations from less than 200 human demonstrations across 18 tasks

# RoboMimic:
#   title: What Matters in Learning from Offline Human Demonstrations for Robot Manipulation
#   tags: [YukuZhu, FeiFei, policy, RoboSuite]
#   paper: https://arxiv.org/pdf/2108.03298
#   website: https://robomimic.github.io/
#   repo: https://github.com/ARISE-Initiative/robomimic
#   notes: A Framework for Robot Learning from Demonstration

Behavior-1k:
  title: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation
  tags: [FeiFei, benchmark, OmniGibson]
  paper: https://arxiv.org/abs/2403.09227
  website: https://behavior.stanford.edu/behavior-1k
  repo: https://github.com/StanfordVL/OmniGibson
  notes: the 1,000 activities come from the results of an extensive survey on what do you want robots to do for you?

# ARCap:
#   title: Collecting High-quality Human Demonstrations for Robot Learning with Augmented Reality Feedback
#   tags: [FeiFei, data collection]
#   paper: https://arxiv.org/abs/2410.08464
#   website: https://stanford-tml.github.io/ARCap/
#   repo: https://github.com/Ericcsr/ARCap
#   notes: a portable data collection system that provides visual feedback through augmented reality (AR) and haptic warnings to guide users in collecting high-quality demonstrations.

DigitalCousin:
  title: Automated Creation of Digital Cousins (ACDC) for Robust Policy Learning
  tags: [FeiFei, data synthesis]
  paper: https://arxiv.org/abs/2410.07408
  website: https://digital-cousins.github.io/
  repo: https://github.com/cremebrule/digital-cousins
  notes: ACDC can produce digital cousin scenes that preserve geometric and semantic affordances, and can be used to train policies that outperform policies trained on digital twins, achieving 90% vs. 25% under zero-shot sim-to-real transfer.

# LeRobot:
#   title: Making AI for Robotics more accessible with end-to-end learning
#   tags: [HuggingFace, policy, dataset]
#   repo: https://github.com/huggingface/lerobot
#   notes: contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.

# Diffusion:
#   title: Visuomotor Policy Learning via Action Diffusion
#   tags: [ShuranSong, policy]
#   paper: https://arxiv.org/abs/2303.04137
#   website: https://diffusion-policy.cs.columbia.edu/
#   repo: https://github.com/real-stanford/diffusion_policy
#   notes: the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability.

# OpenXEmbodiment:
#   title: Robotic Learning Datasets and RT-X Models
#   tags: [DeepMind, dataset, collaboration]
#   paper: https://arxiv.org/abs/2310.08864
#   website: https://robotics-transformer-x.github.io/
#   repo: https://github.com/google-deepmind/open_x_embodiment
#   notes: It contains 1M+ real robot trajectories spanning 22 robot embodiments, from single robot arms to bi-manual robots and quadrupeds.

# DROID:
#   title: A Large-Scale In-the-Wild Robot Manipulation Dataset
#   tags: [DeepMind, dataset, collaboration, robomimic]
#   paper: https://arxiv.org/abs/2403.12945
#   website: https://droid-dataset.github.io/
#   repo: https://droid-dataset.github.io/droid/
#   notes: DROID uses the same hardware setup across all 13 institutions. 76k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks

# UMI:
#   title: Universal Manipulation Interface. In-The-Wild Robot Teaching Without In-The-Wild Robots
#   tags: [ShuranSong, data collection]
#   paper: https://arxiv.org/abs/2402.10329
#   website: https://umi-gripper.github.io/
#   repo: https://github.com/real-stanford/universal_manipulation_interface
#   notes: UMI employs hand-held grippers enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. learned policies are hardware-agnostic and deployable across multiple robot platforms.

ManiWAV:
  title: Learning Robot Manipulation from In-the-Wild Audio-Visual Data
  tags: [ShuranSong, policy, audio-visual]
  paper: https://arxiv.org/abs/2406.19464
  website: https://mani-wav.github.io/
  repo: https://github.com/real-stanford/maniwav
  notes: Incorporating contact audio as additional source of information improves robustness and generalizability of the policy.

# AlohaACT:
#   title: Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware
#   tags: [ChelseaFinn, data collection, policy]
#   paper: https://arxiv.org/abs/2304.13705
#   website: https://tonyzhaozh.github.io/aloha/
#   repo: https://github.com/tonyzhaozh/aloha
#   notes: a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Action Chunking with Transformers (ACT)

# MobileAloha:
#   title: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation
#   tags: [ChelseaFinn, data collection, dataset]
#   paper: https://arxiv.org/abs/2401.02117
#   website: https://mobile-aloha.github.io/
#   repo: https://github.com/MarkFzp/mobile-aloha
#   notes: It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface.

# AlohaUnleashed:
#   title: A Simple Recipe for Robot Dexterity
#   tags: [DeepMind, ChelseaFinn, data collection, policy]
#   paper: https://aloha-unleashed.github.io/assets/aloha_unleashed.pdf
#   website: https://aloha-unleashed.github.io/
#   notes: a simple recipe of large scale data collection, combined with expressive models such as Diffusion Policies, can be effective in learning challenging bimanual manipulation tasks involving deformable objects and complex contact rich dynamics.

# ManiSkill:
#   title: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI
#   tags: [HaoSu, benchmark, simulation]
#   paper: https://arxiv.org/abs/2410.00425
#   website: https://www.maniskill.ai/
#   repo: https://github.com/haosulab/ManiSkill
#   notes: 40+ skills/tasks with 2,000+ objects ready-to-use, millions of frames of demonstrations available, and dense reward functions.

# TDMPC2:
#   title: Scalable, Robust World Models for Continuous Control
#   tags: [HaoSu, policy]
#   paper: https://arxiv.org/abs/2310.16828
#   website: https://www.tdmpc2.com/
#   repo: https://github.com/nicklashansen/tdmpc2
#   notes: agent capabilities increase with model and data size, and successfully train a single agent to perform 80 tasks across multiple task domains, embodiments, and action spaces

# RFCL:
#   title: Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in RL
#   tags: [HaoSu, policy]
#   paper: https://arxiv.org/abs/2405.03379
#   website: https://reverseforward-cl.github.io/
#   repo: https://github.com/stonet2000/rfcl
#   notes: RFCL is capable of solving a wide range of complex tasks from just 1-10 demonstrations, far more demonstration efficient than prior model-free baselines.

SIMPLER:
  title: Evaluating Real-World Robot Manipulation Policies in Simulation
  tags: [DeepMind, HaoSu, ChelseaFinn, benchmark]
  paper: https://arxiv.org/pdf/2405.05941
  website: https://simpler-env.github.io/
  repo: https://github.com/simpler-env/SimplerEnv
  notes: demonstrate strong correlation between policy performance in SIMPLER environments and in the real world

# OpenVLA:
#   title: An Open-Source Vision-Language-Action Model
#   tags: [DeepMind, ChelseaFinn, policy]
#   paper: https://arxiv.org/abs/2406.09246
#   website: https://openvla.github.io/
#   repo: https://github.com/openvla/openvla
#   notes: a 7B parameter open-source vision-language-action model (VLA), pretrained on 970k robot episodes from the Open X-Embodiment dataset.

LAPA:
  title: Latent Action Pretraining from Videos
  tags: [NVIDIA, KAIST, policy]
  paper: https://arxiv.org/abs/2410.11758
  website: https://latentactionpretraining.github.io/
  repo: https://github.com/LatentActionPretraining/LAPA
  notes: a method to learn from internet-scale videos that do not have robot action labels. 

UMIonLegs:
  title: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers
  tags: [ShuranSong, policy]
  paper: https://arxiv.org/abs/2407.10353
  website: https://umi-on-legs.github.io/
  repo: https://github.com/real-stanford/umi-on-legs
  notes: robot data without robots, task tracking without simulating tasks, combining real-world human demonstrations with simulation trained whole-body controllers, providing a scalable approach for manipulation skills on robot dogs with arms.

# HumanoidBench:
#   title: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation
#   tags: [PieterAbbeel, benchmark, policy, RL, hierarchical]
#   paper: https://arxiv.org/abs/2403.10506
#   website: https://humanoid-bench.github.io/
#   repo: https://github.com/carlosferrazza/humanoid-bench
#   notes: featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks.

HumanoidMPC:
  title: MuJoCo MPC for Humanoid Control, Evaluation on HumanoidBench
  tags: [policy, mpc]
  paper: https://arxiv.org/pdf/2408.00342
  repo: https://github.com/MoritzMeser/mujoco_mpc
  notes: sparse reward functions of HumanoidBench yield undesirable and unrealistic behaviors when optimized. enabling easy experimentation with MPC for humanoid robot control in simulation

iDP3:
  title: Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies
  tags: [JaijunWu, policy, data collection]
  paper: https://arxiv.org/abs/2410.10803
  website: https://humanoid-manipulation.github.io/
  repo: https://github.com/YanjieZe/Improved-3D-Diffusion-Policy
  notes: iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Apple Vision Pro to build a whole-upper-body teleoperation system.

# JetsonAILab:
#   title: NVIDIA Jetson AI Lab
#   tags: [NVIDIA, Jetson, edge deploy]
#   website: https://www.jetson-ai-lab.com/
#   repo: https://github.com/dusty-nv/jetson-containers
#   notes: The Jetson AI Lab Research Group is a global collective for advancing open-source Edge ML. Containers provided by jetson-containers.

# VISTA:
#   title: View-Invariant Policy Learning via Zero-Shot Novel View Synthesis
#   tags: [Jiajun Wu, data augmentation]
#   paper: https://arxiv.org/abs/2409.03685
#   website: https://s-tian.github.io/projects/vista/
#   repo: https://github.com/s-tian/VISTA
#   notes: VISTA uses zero-shot novel view synthesis models, which generate novel views of an input image given a specified camera transformation, to create synthetic images for data augmentation. ZeroNVS has strong zero-shot performance on diverse data.

Gen2Act:
  title: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation
  tags: [DeepMind, policy, out-of-distribution]
  paper: https://arxiv.org/abs/2409.16283
  website: https://homangab.github.io/gen2act/
  notes: Gen2Act first imagines how a human would perform the task through video generation with a pre-trained model, and then executes a common policy conditioned on the generated video.

GenSim2:
  title: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs
  tags: [simulation, data generation, policy, ReKep]
  paper: https://arxiv.org/abs/2410.03645
  website: https://gensim2.github.io/
  repo: https://github.com/GenSim2/gensim2
  notes: uses multimodal LLMs to generate vast amounts of articulated, 6-dof robotic tasks in simulation for pre-training a generalist 3D multitask policies. The framework "amplifies" limited real world tasks and trajectories with foundation models.

# RUM:
#   title: Robot Utility Models, General Policies for Zero-Shot Deployment in New Environments
#   tags: [HelloRobot, LerrelPinto, policy]
#   paper: https://arxiv.org/abs/2409.05865
#   website: https://robotutilitymodels.com/
#   repo: https://github.com/haritheja-e/robot-utility-models/
#   notes: We feed in a summary of robot observations into a multimodal LLM, which determines whether or not the task at hand has succeeded. If the mLLM determines that the task has failed, the robot automatically resets to a new initial state and retries.

ROSA:
  title: The ROS Agent
  tags: [ROS, policy]
  repo: https://github.com/nasa-jpl/rosa
  paper: https://arxiv.org/abs/2410.06472
  notes: ROSA is your AI-powered assistant for ROS1 and ROS2 systems. Built on the Langchain framework, ROSA helps you interact with robots using natural language, making robotics development more accessible and efficient.

OKAMI:
  title: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation
  tags: [NVIDIA, YukeZhu, policy]
  paper: https://arxiv.org/abs/2410.11792
  website: https://ut-austin-rpl.github.io/OKAMI/
  notes:  it retargets the SMPL-H trajectory to the humanoid, using inverse kinematics and dex-retargeting. The trajectory is warped based on test-time object's locations, and then sent to the real robot for execution.

SGCRL:
  title: A Single Goal is All You Need. Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals
  tags: [policy, RL, hard exploration, contrastive RL, no reward]
  paper: https://arxiv.org/abs/2408.05804
  website: https://graliuce.github.io/sgcrl/
  repo: https://github.com/graliuce/sgcrl/tree/main
  notes: empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. robot manipulation, maze. Try this with PPO

# PALO:
#   title: Policy Adaptation via Language Optimization. Decomposing Tasks for Few-Shot Imitation
#   tags: [SergeyLevine, policy, generalist]
#   paper: https://arxiv.org/abs/2408.16228
#   website: https://palo-website.github.io/
#   repo: https://github.com/vivekmyers/palo
#   notes: combines a handful of demonstrations of a task with proposed language decompositions sampled from a VLM to quickly enable rapid nonparametric adaptation, avoiding the need for a larger fine-tuning dataset.

MaskedMimic:
  title: Unified Physics-Based Character Control Through Masked Motion Inpainting
  tags: [NVIDIA, policy, controller, no reward]
  paper: https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf
  website: https://research.nvidia.com/labs/par/maskedmimic/
  repo: https://github.com/NVlabs/ProtoMotions
  notes: formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions. leveraging motion tracking data

RoboPoint:
  title: A Vision-Language Model for Spatial Affordance Prediction for Robotics
  tags: [NVIDIA, DieterFox, affordance]
  paper: https://arxiv.org/pdf/2406.10721
  website: https://robo-point.github.io/
  repo: https://github.com/wentaoyuan/RoboPoint
  notes: a VLM that predicts image keypoint affordances given language instructions. a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance.

DynaMo:
  title: In-Domain Dynamics Pretraining for Visuo-Motor Control
  tags: [LerrelPinto, policy, representation]
  paper: https://arxiv.org/abs/2409.12192
  website: https://dynamo-ssl.github.io/
  repo: https://github.com/jeffacce/dynamo_ssl
  notes: we can learn a good visual representation for control by modeling the temporal dynamics on demonstration observations, We model the actions as unobserved latents, and train all models end-to-end with a consistency loss on the forward dynamics prediction.

BAKU:
  title: An Efficient Transformer for Multi-Task Policy Learning
  tags: [LerrelPinto, policy]
  paper: https://arxiv.org/abs/2406.07539
  website: https://baku-robot.github.io/
  repo: https://github.com/siddhanthaldar/BAKU/tree/main
  notes: BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work. 

# DialMPC:
#   title: Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing
#   tags: [policy, MPC]
#   paper: https://arxiv.org/abs/2409.15610
#   website: https://lecar-lab.github.io/dial-mpc/
#   repo: https://github.com/LeCAR-Lab/dial-mpc
#   notes: sampling-based MPC framework for legged robot full-order torque-level control with both precision and agility in a training-free manner.  you can test out the controller in a plug-and-play manner with minimum setup. 

Diff-Control:
  title: Enabling Stateful Behaviors for Diffusion-based Policy Learning
  tags: [policy, diffusion]
  paper: https://arxiv.org/abs/2404.12539
  website: https://diff-control.github.io/
  repo: https://github.com/ir-lab/Diff-Control
  notes: Diff-Control Policy incorporates ControlNet, functioning as a transition model that captures temporal transitions within the action space to ensure action consistency. shows consistent performance in the presence of perturbations, 

FLaRe: 
  title: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning
  tags: [policy, RL, fine-tuning]
  paper: https://arxiv.org/abs/2409.16578
  website: https://robot-flare.github.io/
  notes: stabilize the RL training process, including 1) fine-tuning from a multi-task robotics policy, 2) large-scale fine-tuning in simulation, 3) using an on-policy algorithm (PPO), 4) utilizing smaller learning rate than when performing RL from scratch, 5) disabling the entropy bonus objective, and 6) separating the actor and the critic network, so that the critic update will not influence the policy prediction.

PremierTACO:
  title: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
  tags: [policy, RL, contrastive RL, representation]
  paper: https://arxiv.org/abs/2402.06187
  website: https://premiertaco.github.io/
  repo: https://github.com/PremierTACO/premier-taco
  notes: the temporal action con- trastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, Premier-TACO selects the negative example from a window within the same episode centered at positive example

StreamDiff:
  title: Streaming Diffusion Policy. Fast Policy Synthesis with Variable Noise Diffusion Models
  tags: [policy, diffusion]
  paper: https://arxiv.org/abs/2406.04806
  website: https://streaming-diffusion-policy.github.io/
  repo: https://github.com/Streaming-Diffusion-Policy/streaming_diffusion_policy
  notes: generating a partially denoised action trajectory is substantially faster than a full output action trajectory. dramatically speeding up policy synthesis while preserving performance 

BYOVLA:
  title: Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust
  tags: [policy, runtime intervention]
  paper: https://arxiv.org/abs/2410.01971
  website: https://aasherh.github.io/byovla/
  repo: https://github.com/irom-lab/byovla
  notes: BYOVLA is predicated on three simple steps applied to a VLA's input image. 1) determine task-irrelevant regions, 2) quantify sensitivity by perturbing regions, and 3) transform the image. Task-irrelevant regions are determined by a vision-language model (VLM).

DiTBlock:
  title: The Ingredients for Robotic Diffusion Transformers
  tags: [SergeyLevine, policy, diffusion, dataset]
  paper: https://arxiv.org/abs/2410.10088
  website: https://dit-policy.github.io/
  repo: https://github.com/sudeepdasari/dit-policy
  notes: makes a few simple, yet impactful, changes to the vanilla diffusion transformer recipe. add adaLN-zero layers to the transformer blocks and further optimize the input encoding layers. we collected and annotated BiPlay, a more diverse bi-manual manipulation dataset. robosuite

DataScaling:
  title: Data Scaling Laws in Imitation Learning for Robotic Manipulation
  tags: [dataset, UMI]
  paper: https://arxiv.org/abs/2410.18647
  website: https://data-scaling-laws.github.io/
  repo: https://github.com/Fanqi-Lin/Data-Scaling-Laws
  notes: with proper data scaling, a single-task policy can generalize well to any new environment and any new object within the same category. The diversity of environments and objects is far more important than the absolute number of demonstrations

VisuoSkin:
  title: Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins
  tags: [LerrelPinto, policy, tactile]
  paper: https://arxiv.org/abs/2410.17246
  website: https://visuoskin.github.io/
  repo: https://github.com/raunaqbhirangi/visuoskin
  notes: learning policies with magnetic skin sensors, simple framework that uses a transformer-based policy and treats skin sensor data as additional tokens to vision-based information. 

BUMBLE:
  title: Unifying Reasoning and Acting with VLMs for Building-Wide Mobile Manipulation
  tags: [YukeZhu, policy, vlm]
  paper: https://arxiv.org/abs/2410.06237
  website: https://robin-lab.cs.utexas.edu/BUMBLE/
  repo: https://github.com/UT-Austin-RobIn/BUMBLE
  notes: a unified VLM-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. ong-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial.

HIL-SERL:
  title: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning
  tags: [SergeyLevine, policy, RL]
  paper: https://hil-serl.github.io/static/hil-serl-paper.pdf
  website: https://hil-serl.github.io/
  repo: https://github.com/rail-berkeley/hil-serl
  notes: a set of libraries, env wrappers, and examples to train RL policies using a combination of demonstrations and human corrections to perform robotic manipulation tasks with near-perfect success rates.

# LIBERO:
#   title: Benchmarking Knowledge Transfer for Lifelong Robot Learning
#   tags: [YukeZhu, benchmark]
#   paper: https://arxiv.org/abs/2306.03310
#   website: https://libero-project.github.io/main.html
#   repo: https://github.com/Lifelong-Robot-Learning/LIBERO
#   notes: LIBERO highlights five key research topics in LLDM. 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM

HOVER:
  title: Versatile Neural Whole-Body Controller for Humanoid Robots
  tags: [NVIDIA, YukuZhu, policy, humanoid control]
  paper: https://arxiv.org/abs/2410.21229
  website: https://hover-versatile-humanoid.github.io/
  notes: full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control.  The versatile multi-mode command space supports kinematic position tracking (blue), local joint angle tracking (yellow), and root tracking (purple). 

ManiCentricRepr:
  title: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets
  tags: [policy, representation, pre-training, contrastive loss, action prediction]
  paper: https://arxiv.org/abs/2410.22325
  website: https://robots-pretrain-robots.github.io/
  repo: https://github.com/luccachiang/robots-pretrain-robots]
  notes: Manipulation Centricity measures how well pre-trained visual representations correlate with downstream manipulation tasks, serving as a strong predictor of task success rates. Manipulation Centric Representation (MCR) enhances manipulation centricity by pre-training visual encoders with large-scale robotic data. DROID

DexMimicGen:
  title: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning
  tags: [NVIDIA, YukeZhu, policy, dataset, bimanual]
  paper: https://arxiv.org/abs/2410.24185
  website: https://dexmimicgen.github.io/
  notes:  We used DexMimicGen to autonomously generate over 20,000 demonstrations for bimanual dexterous robots from just 60 source human demonstrations across 9 tasks, multiple simulators, and the real-world.

EgoMimic:
  title: Scaling Imitation Learning through Egocentric Video
  tags: [policy, data collection]
  paper: https://arxiv.org/abs/2410.24221
  website: https://egomimic.github.io/
  repo: https://github.com/SimarKareer/EgoMimic
  notes: a full-stack framework that scales manipulation through egocentric-view human demonstrations. human and robot data equally as embodied demonstration data and learns a unified policy from both data sources. scaling 1 hour of additional hand data is significantly more valuable than 1 hour of additional robot data. 

3D-ViTac:
  title: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing
  tags: [policy, tactile]
  paper: https://arxiv.org/abs/2410.24091
  website: https://binghao-huang.github.io/3D-ViTac/
  notes: To integrate tactile and visual data, we fuse them into a unified 3D representation space that preserves their 3D structures and spatial relationships. The multi-modal representation can then be coupled with diffusion policies for imitation learning. even low-cost robots can perform precise manipulations and significantly outperform vision-only policies 

Im2Flow2Act:
  title: Flow as the Cross-domain Manipulation Interface
  tags: [ShuranSong, policy, flow-conditioned]
  paper: https://arxiv.org/abs/2407.15208
  website: https://im-flow-act.github.io/
  repo: https://github.com/real-stanford/im2Flow2Act
  notes: The flow generation network, trained on human demonstration videos, generates object flow from the initial scene image, conditioned on the task description. The flow-conditioned policy, trained on simulated robot play data, maps the generated object flow to robot actions to realize the desired object movements. By using flow as input, this policy can be directly deployed in the real world with a minimal sim-to-real gap.

FlowBotHD:
  title: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation 
  tags: [policy, representation]
  paper: https://arxiv.org/abs/2410.07078
  website: https://flowbothd.github.io/
  repo: https://github.com/liy1shu/FlowBotHD
  notes: we propose a history-aware diffusion network that models the multi-modal distribution of the articulated object and uses history to disambiguate actions and make stable predictions under occlusions.

SiriusFleet:
  title: Multi-Task Interactive Robot Fleet Learning with Visual World Models
  tags: [YukeZhu, policy, multi-agent, world model]
  paper: https://arxiv.org/abs/2410.22689
  website: https://ut-austin-rpl.github.io/sirius-fleet/
  notes: We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. 

RT-Affordance:
  title: Affordances are Versatile Intermediate Representations for Robot Manipulation
  tags: [policy, representation, affordance]
  paper: https://arxiv.org/abs/2411.02704
  website: https://snasiriany.me/rt-affordance
  notes: Conditioning on language is intuitive, yet language typically does not provide enough guidance on how to perform the task. Goal images and trajectory sketches are over-specified and present learning challenges. We propose conditioning policies on intermediate affordance representations, which are expressive yet compact representations of tasks, making them easy to specify and to learn. 

EmbodiedAgentInterface:
  title: Benchmarking LLMs for Embodied Decision Making
  tags: [FeiFei, JiajunWu, benchmark, LLM]
  paper: https://arxiv.org/abs/2410.07166
  website: https://embodied-agent-interface.github.io/
  repo: https://github.com/embodied-agent-interface/embodied-agent-interface
  notes: tackle the following challenges in evaluating LLMs for building embodied decision-making agents. (1) Standardization of goal specifications. (2) Standardization of modules and interfaces. (3) Broad coverage of evaluation and fine-grained metrics. 

CloSD:
  title: Closing the Loop between Simulation and Diffusion for multi-task character control
  tags: [NVIDIA, policy, hierarchical, multi-task]
  paper: https://arxiv.org/abs/2410.03441
  website: https://guytevet.github.io/CLoSD-page/
  repo: https://github.com/GuyTevet/CLoSD
  notes: motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment.

DrRobot:
  title: Differentiable Robot Rendering
  tags: [ShuranSong, policy]
  paper: https://arxiv.org/abs/2410.13851
  website: https://drrobot.cs.columbia.edu/
  repo: https://github.com/cvlab-columbia/drrobot
  notes: demonstrate its capability and usage in applications including reconstruction of robot poses from images and controlling robots through vision language models
