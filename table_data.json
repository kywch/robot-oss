{
  "title": "Robotics open-source projects (2024/10)",
  "texts": [
    {
      "id": 1730160469747,
      "content": "Improved policy + RL fine-tuning",
      "x": 135,
      "y": 17,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 115
    },
    {
      "id": 1730160616868,
      "content": "UMI: train on real, test on sim (single-hand, yet)",
      "x": 623.2222222222222,
      "y": 19.11111111111111,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 440
    },
    {
      "id": 1730160888303,
      "content": "Human video -> humanoid",
      "x": 125,
      "y": 356,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 403
    },
    {
      "id": 1730161067736,
      "content": "New task/demo/asset generation + ideas",
      "x": 640.6666666666666,
      "y": 143.2222222222222,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 443
    },
    {
      "id": 1730161528828,
      "content": "Interesting VLM use cases",
      "x": 719.3333333333331,
      "y": 263.7777777777778,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 388
    },
    {
      "id": 1730161617781,
      "content": "Impressive manipulation demos: ",
      "x": 563.1111111111111,
      "y": 422.66666666666663,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 21,
      "zIndex": 421
    },
    {
      "id": 1730323231942,
      "content": "New modality/representation",
      "x": 64,
      "y": 195,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 16,
      "zIndex": 433
    }
  ],
  "cards": [
    {
      "key": "ReKep",
      "title": "Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation",
      "tags": [
        "FeiFei",
        "policy",
        "hierarchical",
        "OmniGibson"
      ],
      "paper": "https://arxiv.org/abs/2409.01652",
      "website": "https://rekep-robot.github.io/",
      "repo": "https://github.com/huangwl18/ReKep",
      "notes": "Large vision models and vision-language models can generate keypoint-based constraints, which can be optimized to achieve multi-stage, in-the-wild, bimanual, and reactive behaviors, without task-specific training or environment models.",
      "x": 597,
      "y": 188.1111111111111,
      "zIndex": 384
    },
    {
      "key": "RoboCasa",
      "title": "Large-Scale Simulation of Everyday Tasks for Generalist Robots",
      "tags": [
        "NVIDIA",
        "YukeZhu",
        "benchmark",
        "dataset",
        "RoboSuite"
      ],
      "paper": "https://arxiv.org/abs/2406.02523",
      "website": "https://robocasa.ai/",
      "repo": "https://github.com/robocasa/robocasa",
      "notes": "2,500 3D assets across 150+ object categories and dozens of interactable furniture and appliances. A suite of 100 everyday activities with 100k demonstrations.",
      "x": 907,
      "y": 67,
      "zIndex": 438
    },
    {
      "key": "Behavior-1k",
      "title": "A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation",
      "tags": [
        "FeiFei",
        "benchmark",
        "OmniGibson"
      ],
      "paper": "https://arxiv.org/abs/2403.09227",
      "website": "https://behavior.stanford.edu/behavior-1k",
      "repo": "https://github.com/StanfordVL/OmniGibson",
      "notes": "the 1,000 activities come from the results of an extensive survey on what do you want robots to do for you?",
      "x": 1061.5555555555557,
      "y": 185.1111111111111,
      "zIndex": 442
    },
    {
      "key": "DigitalCousin",
      "title": "Automated Creation of Digital Cousins (ACDC) for Robust Policy Learning",
      "tags": [
        "FeiFei",
        "data synthesis"
      ],
      "paper": "https://arxiv.org/abs/2410.07408",
      "website": "https://digital-cousins.github.io/",
      "repo": "https://github.com/cremebrule/digital-cousins",
      "notes": "ACDC can produce digital cousin scenes that preserve geometric and semantic affordances, and can be used to train policies that outperform policies trained on digital twins, achieving 90% vs. 25% under zero-shot sim-to-real transfer.",
      "x": 910,
      "y": 186,
      "zIndex": 386
    },
    {
      "key": "ManiWAV",
      "title": "Learning Robot Manipulation from In-the-Wild Audio-Visual Data",
      "tags": [
        "ShuranSong",
        "policy",
        "audio-visual"
      ],
      "paper": "https://arxiv.org/abs/2406.19464",
      "website": "https://mani-wav.github.io/",
      "repo": "https://github.com/real-stanford/maniwav",
      "notes": "Incorporating contact audio as additional source of information improves robustness and generalizability of the policy.",
      "x": 36,
      "y": 229,
      "zIndex": 434
    },
    {
      "key": "SIMPLER",
      "title": "Evaluating Real-World Robot Manipulation Policies in Simulation",
      "tags": [
        "DeepMind",
        "HaoSu",
        "ChelseaFinn",
        "benchmark"
      ],
      "paper": "https://arxiv.org/pdf/2405.05941",
      "website": "https://simpler-env.github.io/",
      "repo": "https://github.com/simpler-env/SimplerEnv",
      "notes": "demonstrate strong correlation between policy performance in SIMPLER environments and in the real world",
      "x": 1059,
      "y": 68,
      "zIndex": 439
    },
    {
      "key": "LAPA",
      "title": "Latent Action Pretraining from Videos",
      "tags": [
        "NVIDIA",
        "KAIST",
        "policy"
      ],
      "paper": "https://arxiv.org/abs/2410.11758",
      "website": "https://latentactionpretraining.github.io/",
      "repo": "https://github.com/LatentActionPretraining/LAPA",
      "notes": "a method to learn from internet-scale videos that do not have robot action labels.",
      "x": 47,
      "y": 407,
      "zIndex": 402
    },
    {
      "key": "UMIonLegs",
      "title": "Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers",
      "tags": [
        "ShuranSong",
        "policy"
      ],
      "paper": "https://arxiv.org/abs/2407.10353",
      "website": "https://umi-on-legs.github.io/",
      "repo": "https://github.com/real-stanford/umi-on-legs",
      "notes": "robot data without robots, task tracking without simulating tasks, combining real-world human demonstrations with simulation trained whole-body controllers, providing a scalable approach for manipulation skills on robot dogs with arms.",
      "x": 599,
      "y": 67,
      "zIndex": 381
    },
    {
      "key": "HumanoidMPC",
      "title": "MuJoCo MPC for Humanoid Control, Evaluation on HumanoidBench",
      "tags": [
        "policy",
        "mpc"
      ],
      "paper": "https://arxiv.org/pdf/2408.00342",
      "repo": "https://github.com/MoritzMeser/mujoco_mpc",
      "notes": "sparse reward functions of HumanoidBench yield undesirable and unrealistic behaviors when optimized. enabling easy experimentation with MPC for humanoid robot control in simulation",
      "x": 50,
      "y": 478,
      "zIndex": 398
    },
    {
      "key": "iDP3",
      "title": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies",
      "tags": [
        "JaijunWu",
        "policy",
        "data collection"
      ],
      "paper": "https://arxiv.org/abs/2410.10803",
      "website": "https://humanoid-manipulation.github.io/",
      "repo": "https://github.com/YanjieZe/Improved-3D-Diffusion-Policy",
      "notes": "iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Apple Vision Pro to build a whole-upper-body teleoperation system.",
      "x": 187,
      "y": 231,
      "zIndex": 435
    },
    {
      "key": "Gen2Act",
      "title": "Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation",
      "tags": [
        "DeepMind",
        "policy",
        "out-of-distribution"
      ],
      "paper": "https://arxiv.org/abs/2409.16283",
      "website": "https://homangab.github.io/gen2act/",
      "notes": "Gen2Act first imagines how a human would perform the task through video generation with a pre-trained model, and then executes a common policy conditioned on the generated video.",
      "x": 382,
      "y": 407,
      "zIndex": 400
    },
    {
      "key": "GenSim2",
      "title": "Scaling Robot Data Generation with Multi-modal and Reasoning LLMs",
      "tags": [
        "simulation",
        "data generation",
        "policy",
        "ReKep"
      ],
      "paper": "https://arxiv.org/abs/2410.03645",
      "website": "https://gensim2.github.io/",
      "repo": "https://github.com/GenSim2/gensim2",
      "notes": "uses multimodal LLMs to generate vast amounts of articulated, 6-dof robotic tasks in simulation for pre-training a generalist 3D multitask policies. The framework \"amplifies\" limited real world tasks and trajectories with foundation models.",
      "x": 751.5555555555555,
      "y": 187.22222222222223,
      "zIndex": 385
    },
    {
      "key": "ROSA",
      "title": "The ROS Agent",
      "tags": [
        "ROS",
        "policy"
      ],
      "paper": "https://arxiv.org/abs/2410.06472",
      "repo": "https://github.com/nasa-jpl/rosa",
      "notes": "ROSA is your AI-powered assistant for ROS1 and ROS2 systems. Built on the Langchain framework, ROSA helps you interact with robots using natural language, making robotics development more accessible and efficient.",
      "x": 593,
      "y": 317,
      "zIndex": 389
    },
    {
      "key": "OKAMI",
      "title": "Teaching Humanoid Robots Manipulation Skills through Single Video Imitation",
      "tags": [
        "NVIDIA",
        "YukeZhu",
        "policy"
      ],
      "paper": "https://arxiv.org/abs/2410.11792",
      "website": "https://ut-austin-rpl.github.io/OKAMI/",
      "notes": "it retargets the SMPL-H trajectory to the humanoid, using inverse kinematics and dex-retargeting. The trajectory is warped based on test-time object's locations, and then sent to the real robot for execution.",
      "x": 215,
      "y": 406,
      "zIndex": 401
    },
    {
      "key": "SGCRL",
      "title": "A Single Goal is All You Need. Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals",
      "tags": [
        "policy",
        "RL",
        "hard exploration",
        "contrastive RL",
        "no reward"
      ],
      "paper": "https://arxiv.org/abs/2408.05804",
      "website": "https://graliuce.github.io/sgcrl/",
      "repo": "https://github.com/graliuce/sgcrl/tree/main",
      "notes": "empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. robot manipulation, maze. Try this with PPO",
      "x": 377,
      "y": 209,
      "zIndex": 229
    },
    {
      "key": "MaskedMimic",
      "title": "Unified Physics-Based Character Control Through Masked Motion Inpainting",
      "tags": [
        "NVIDIA",
        "policy",
        "controller",
        "no reward"
      ],
      "paper": "https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf",
      "website": "https://research.nvidia.com/labs/par/maskedmimic/",
      "repo": "https://github.com/NVlabs/ProtoMotions",
      "notes": "formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions. leveraging motion tracking data",
      "x": 215,
      "y": 477,
      "zIndex": 397
    },
    {
      "key": "RoboPoint",
      "title": "A Vision-Language Model for Spatial Affordance Prediction for Robotics",
      "tags": [
        "NVIDIA",
        "DieterFox",
        "affordance"
      ],
      "paper": "https://arxiv.org/pdf/2406.10721",
      "website": "https://robo-point.github.io/",
      "repo": "https://github.com/wentaoyuan/RoboPoint",
      "notes": "a VLM that predicts image keypoint affordances given language instructions. a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance.",
      "x": 902.7777777777777,
      "y": 318.11111111111114,
      "zIndex": 391
    },
    {
      "key": "DynaMo",
      "title": "In-Domain Dynamics Pretraining for Visuo-Motor Control",
      "tags": [
        "LerrelPinto",
        "policy",
        "representation"
      ],
      "paper": "https://arxiv.org/abs/2409.12192",
      "website": "https://dynamo-ssl.github.io/",
      "repo": "https://github.com/jeffacce/dynamo_ssl",
      "notes": "we can learn a good visual representation for control by modeling the temporal dynamics on demonstration observations, We model the actions as unobserved latents, and train all models end-to-end with a consistency loss on the forward dynamics prediction.",
      "x": 36,
      "y": 289,
      "zIndex": 436
    },
    {
      "key": "BAKU",
      "title": "An Efficient Transformer for Multi-Task Policy Learning",
      "tags": [
        "LerrelPinto",
        "policy"
      ],
      "paper": "https://arxiv.org/abs/2406.07539",
      "website": "https://baku-robot.github.io/",
      "repo": "https://github.com/siddhanthaldar/BAKU/tree/main",
      "notes": "BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work.",
      "x": 38,
      "y": 75,
      "zIndex": 104
    },
    {
      "key": "Diff-Control",
      "title": "Enabling Stateful Behaviors for Diffusion-based Policy Learning",
      "tags": [
        "policy",
        "diffusion"
      ],
      "paper": "https://arxiv.org/abs/2404.12539",
      "website": "https://diff-control.github.io/",
      "repo": "https://github.com/ir-lab/Diff-Control",
      "notes": "Diff-Control Policy incorporates ControlNet, functioning as a transition model that captures temporal transitions within the action space to ensure action consistency. shows consistent performance in the presence of perturbations,",
      "x": 37,
      "y": 142,
      "zIndex": 105
    },
    {
      "key": "FLaRe",
      "title": "Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning",
      "tags": [
        "policy",
        "RL",
        "fine-tuning"
      ],
      "paper": "https://arxiv.org/abs/2409.16578",
      "website": "https://robot-flare.github.io/",
      "notes": "stabilize the RL training process, including 1) fine-tuning from a multi-task robotics policy, 2) large-scale fine-tuning in simulation, 3) using an on-policy algorithm (PPO), 4) utilizing smaller learning rate than when performing RL from scratch, 5) disabling the entropy bonus objective, and 6) separating the actor and the critic network, so that the critic update will not influence the policy prediction.",
      "x": 378,
      "y": 75,
      "zIndex": 227
    },
    {
      "key": "PremierTACO",
      "title": "Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
      "tags": [
        "policy",
        "RL",
        "contrastive RL",
        "representation"
      ],
      "paper": "https://arxiv.org/abs/2402.06187",
      "website": "https://premiertaco.github.io/",
      "repo": "https://github.com/PremierTACO/premier-taco",
      "notes": "the temporal action con- trastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, Premier-TACO selects the negative example from a window within the same episode centered at positive example",
      "x": 378,
      "y": 141,
      "zIndex": 228
    },
    {
      "key": "StreamDiff",
      "title": "Streaming Diffusion Policy. Fast Policy Synthesis with Variable Noise Diffusion Models",
      "tags": [
        "policy",
        "diffusion"
      ],
      "paper": "https://arxiv.org/abs/2406.04806",
      "website": "https://streaming-diffusion-policy.github.io/",
      "repo": "https://github.com/Streaming-Diffusion-Policy/streaming_diffusion_policy",
      "notes": "generating a partially denoised action trajectory is substantially faster than a full output action trajectory. dramatically speeding up policy synthesis while preserving performance",
      "x": 188,
      "y": 139,
      "zIndex": 409
    },
    {
      "key": "BYOVLA",
      "title": "Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust",
      "tags": [
        "policy",
        "runtime intervention"
      ],
      "paper": "https://arxiv.org/abs/2410.01971",
      "website": "https://aasherh.github.io/byovla/",
      "repo": "https://github.com/irom-lab/byovla",
      "notes": "BYOVLA is predicated on three simple steps applied to a VLA's input image. 1) determine task-irrelevant regions, 2) quantify sensitivity by perturbing regions, and 3) transform the image. Task-irrelevant regions are determined by a vision-language model (VLM).",
      "x": 1049.7777777777778,
      "y": 317.0000000000001,
      "zIndex": 392
    },
    {
      "key": "DiTBlock",
      "title": "The Ingredients for Robotic Diffusion Transformers",
      "tags": [
        "SergeyLevine",
        "policy",
        "diffusion",
        "dataset"
      ],
      "paper": "https://arxiv.org/abs/2410.10088",
      "website": "https://dit-policy.github.io/",
      "repo": "https://github.com/sudeepdasari/dit-policy",
      "notes": "makes a few simple, yet impactful, changes to the vanilla diffusion transformer recipe. add adaLN-zero layers to the transformer blocks and further optimize the input encoding layers. we collected and annotated BiPlay, a more diverse bi-manual manipulation dataset. robosuite",
      "x": 190,
      "y": 75,
      "zIndex": 107
    },
    {
      "key": "DataScaling",
      "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation",
      "tags": [
        "dataset",
        "UMI"
      ],
      "paper": "https://arxiv.org/abs/2410.18647",
      "website": "https://data-scaling-laws.github.io/",
      "repo": "https://github.com/Fanqi-Lin/Data-Scaling-Laws",
      "notes": "with proper data scaling, a single-task policy can generalize well to any new environment and any new object within the same category. The diversity of environments and objects is far more important than the absolute number of demonstrations",
      "x": 751,
      "y": 68,
      "zIndex": 382
    },
    {
      "key": "VisuoSkin",
      "title": "Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins",
      "tags": [
        "LerrelPinto",
        "policy",
        "tactile"
      ],
      "paper": "https://arxiv.org/abs/2410.17246",
      "website": "https://visuoskin.github.io/",
      "repo": "https://github.com/raunaqbhirangi/visuoskin",
      "notes": "learning policies with magnetic skin sensors, simple framework that uses a transformer-based policy and treats skin sensor data as additional tokens to vision-based information.",
      "x": 907.2222222222223,
      "y": 421.66666666666674,
      "zIndex": 417
    },
    {
      "key": "BUMBLE",
      "title": "Unifying Reasoning and Acting with VLMs for Building-Wide Mobile Manipulation",
      "tags": [
        "YukeZhu",
        "policy",
        "vlm"
      ],
      "paper": "https://arxiv.org/abs/2410.06237",
      "website": "https://robin-lab.cs.utexas.edu/BUMBLE/",
      "repo": "https://github.com/UT-Austin-RobIn/BUMBLE",
      "notes": "a unified VLM-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. ong-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial.",
      "x": 745.5555555555555,
      "y": 317.55555555555554,
      "zIndex": 390
    },
    {
      "key": "HIL-SERL",
      "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning",
      "tags": [
        "SergeyLevine",
        "policy",
        "RL"
      ],
      "paper": "https://hil-serl.github.io/static/hil-serl-paper.pdf",
      "website": "https://hil-serl.github.io/",
      "repo": "https://github.com/rail-berkeley/hil-serl",
      "notes": "a set of libraries, env wrappers, and examples to train RL policies using a combination of demonstrations and human corrections to perform robotic manipulation tasks with near-perfect success rates.",
      "x": 1055.888888888889,
      "y": 422.1111111111111,
      "zIndex": 418
    },
    {
      "key": "HOVER",
      "title": "Versatile Neural Whole-Body Controller for Humanoid Robots",
      "tags": [
        "NVIDIA",
        "YukuZhu",
        "policy",
        "humanoid control"
      ],
      "paper": "https://arxiv.org/abs/2410.21229",
      "website": "https://hover-versatile-humanoid.github.io/",
      "notes": "full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control.  The versatile multi-mode command space supports kinematic position tracking (blue), local joint angle tracking (yellow), and root tracking (purple).",
      "x": 381,
      "y": 477.75,
      "zIndex": 399
    },
    {
      "key": "ManiCentricRepr",
      "title": "Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets",
      "tags": [
        "policy",
        "representation",
        "pre-training",
        "contrastive loss",
        "action prediction"
      ],
      "paper": "https://arxiv.org/abs/2410.22325",
      "website": "https://robots-pretrain-robots.github.io/",
      "repo": "https://github.com/luccachiang/robots-pretrain-robots]",
      "notes": "Manipulation Centricity measures how well pre-trained visual representations correlate with downstream manipulation tasks, serving as a strong predictor of task success rates. Manipulation Centric Representation (MCR) enhances manipulation centricity by pre-training visual encoders with large-scale robotic data. DROID",
      "x": 187.25,
      "y": 290.25,
      "zIndex": 437
    }
  ],
  "zoom": 1
}